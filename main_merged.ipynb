{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7991539,"sourceType":"datasetVersion","datasetId":4704710}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Regression Used Cars Dataset\n\nhttps://www.kaggle.com/datasets/austinreese/craigslist-carstrucks-data \n","metadata":{}},{"cell_type":"markdown","source":"#### **Dependency**","metadata":{}},{"cell_type":"code","source":"#! pip install lightgbm\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom itertools import combinations  # For creating combinations of elements\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV,train_test_split\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\nimport lightgbm as lgb \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cluster import KMeans\nfrom sklearn import preprocessing","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:18:39.132160Z","iopub.execute_input":"2024-04-05T14:18:39.132580Z","iopub.status.idle":"2024-04-05T14:18:44.106125Z","shell.execute_reply.started":"2024-04-05T14:18:39.132545Z","shell.execute_reply":"2024-04-05T14:18:44.104800Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from itertools import combinations  # For creating combinations of elements\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cluster import KMeans\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:18:52.942972Z","iopub.execute_input":"2024-04-05T14:18:52.944175Z","iopub.status.idle":"2024-04-05T14:18:53.060577Z","shell.execute_reply.started":"2024-04-05T14:18:52.944134Z","shell.execute_reply":"2024-04-05T14:18:53.059236Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"#### **Utility Function**","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n    # Calculate the initial memory usage of the DataFrame\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    # 🔄 Iterate through each column in the DataFrame\n    for col in df.columns:\n        col_type = df[col].dtype\n        # Check if the column's data type is not 'object' (i.e., numeric)\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            # Check if the column's data type is an integer\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # Check if the column's data type is a float\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n    # ℹ️ Provide memory optimization information if 'verbose' is True\n    if verbose:\n        print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        print(f\"Decreased by {decrease:.2f}%\")\n\n    # Return the DataFrame with optimized memory usage\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:18:58.186891Z","iopub.execute_input":"2024-04-05T14:18:58.187338Z","iopub.status.idle":"2024-04-05T14:18:58.207577Z","shell.execute_reply.started":"2024-04-05T14:18:58.187299Z","shell.execute_reply":"2024-04-05T14:18:58.205783Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### **Load Data**\n\nload csv inside data folder","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/vehicles/vehicles.csv\")\ndf=reduce_mem_usage(df,verbose=True)\ndf.shape\ndf[0:4]","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:31:55.289126Z","iopub.execute_input":"2024-04-05T15:31:55.289676Z","iopub.status.idle":"2024-04-05T15:32:24.448900Z","shell.execute_reply.started":"2024-04-05T15:31:55.289635Z","shell.execute_reply":"2024-04-05T15:32:24.447300Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Memory usage of dataframe is 84.68 MB\nMemory usage after optimization is: 76.54 MB\nDecreased by 9.62%\n","output_type":"stream"},{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"           id                                                url  \\\n0  7222695916  https://prescott.craigslist.org/cto/d/prescott...   \n1  7218891961  https://fayar.craigslist.org/ctd/d/bentonville...   \n2  7221797935  https://keys.craigslist.org/cto/d/summerland-k...   \n3  7222270760  https://worcester.craigslist.org/cto/d/west-br...   \n\n                   region                        region_url  price  year  \\\n0                prescott   https://prescott.craigslist.org   6000   NaN   \n1            fayetteville      https://fayar.craigslist.org  11900   NaN   \n2            florida keys       https://keys.craigslist.org  21000   NaN   \n3  worcester / central MA  https://worcester.craigslist.org   1500   NaN   \n\n  manufacturer model condition cylinders fuel  odometer title_status  \\\n0          NaN   NaN       NaN       NaN  NaN       NaN          NaN   \n1          NaN   NaN       NaN       NaN  NaN       NaN          NaN   \n2          NaN   NaN       NaN       NaN  NaN       NaN          NaN   \n3          NaN   NaN       NaN       NaN  NaN       NaN          NaN   \n\n  transmission  VIN drive size type paint_color image_url description  county  \\\n0          NaN  NaN   NaN  NaN  NaN         NaN       NaN         NaN     NaN   \n1          NaN  NaN   NaN  NaN  NaN         NaN       NaN         NaN     NaN   \n2          NaN  NaN   NaN  NaN  NaN         NaN       NaN         NaN     NaN   \n3          NaN  NaN   NaN  NaN  NaN         NaN       NaN         NaN     NaN   \n\n  state  lat  long posting_date  \n0    az  NaN   NaN          NaN  \n1    ar  NaN   NaN          NaN  \n2    fl  NaN   NaN          NaN  \n3    ma  NaN   NaN          NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>url</th>\n      <th>region</th>\n      <th>region_url</th>\n      <th>price</th>\n      <th>year</th>\n      <th>manufacturer</th>\n      <th>model</th>\n      <th>condition</th>\n      <th>cylinders</th>\n      <th>fuel</th>\n      <th>odometer</th>\n      <th>title_status</th>\n      <th>transmission</th>\n      <th>VIN</th>\n      <th>drive</th>\n      <th>size</th>\n      <th>type</th>\n      <th>paint_color</th>\n      <th>image_url</th>\n      <th>description</th>\n      <th>county</th>\n      <th>state</th>\n      <th>lat</th>\n      <th>long</th>\n      <th>posting_date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7222695916</td>\n      <td>https://prescott.craigslist.org/cto/d/prescott...</td>\n      <td>prescott</td>\n      <td>https://prescott.craigslist.org</td>\n      <td>6000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>az</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7218891961</td>\n      <td>https://fayar.craigslist.org/ctd/d/bentonville...</td>\n      <td>fayetteville</td>\n      <td>https://fayar.craigslist.org</td>\n      <td>11900</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ar</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7221797935</td>\n      <td>https://keys.craigslist.org/cto/d/summerland-k...</td>\n      <td>florida keys</td>\n      <td>https://keys.craigslist.org</td>\n      <td>21000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>fl</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7222270760</td>\n      <td>https://worcester.craigslist.org/cto/d/west-br...</td>\n      <td>worcester / central MA</td>\n      <td>https://worcester.craigslist.org</td>\n      <td>1500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ma</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### **Split dataset** ","metadata":{}},{"cell_type":"code","source":"df_train, df_test = train_test_split(df, test_size=0.2)\nprint(df_train.shape, df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:32:32.081202Z","iopub.execute_input":"2024-04-05T15:32:32.081832Z","iopub.status.idle":"2024-04-05T15:32:32.919545Z","shell.execute_reply.started":"2024-04-05T15:32:32.081783Z","shell.execute_reply":"2024-04-05T15:32:32.918278Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"(341504, 26) (85376, 26)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **Data Function**\n\ndata preperation\n\nfeature engineering","metadata":{}},{"cell_type":"code","source":"def data_preparation(df:pd.DataFrame)->pd.DataFrame:\n    \"\"\"\"\n    make data valid\n    \"\"\"\n    #The professional seller has posted the same cars for several times, they just modify the state info,but we must remove these duplicated rows.\n    #Therefore, using the manufacturer, model, price, year, and VIN as reference values, we will proceed to remove all duplicates we find\n    df.drop_duplicates(subset=[\"price\", \"year\", \"manufacturer\", \"model\", \"VIN\"], keep=\"first\", inplace=True)\n\n    #Create df1, get the average value of each year with deleting NAN and outliers, then assign it to the NAN in df, \n    #and then delete the outliers in df to get the latest data set\n    df1 = df.copy()\n    odometer_data = df1['odometer']\n    odometer_data_cleaned = odometer_data.dropna()\n    kmeans = KMeans(n_clusters=4)\n    kmeans.fit(odometer_data_cleaned.values.reshape(-1, 1))\n    outliers = odometer_data_cleaned[kmeans.labels_ == 1]\n    df1 = df1[~df1['odometer'].isin(outliers)]\n\n    df = df.reset_index(drop=True)\n    df1 = df1.reset_index(drop=True)\n    df = df.drop_duplicates(subset='year')\n    df1 = df1.drop_duplicates(subset='year')\n    df1['avg_odometer'] = df1.groupby('year')['odometer'].transform('mean')\n    df['odometer'] = df['odometer'].fillna(df['year'].map(df1.set_index('year')['avg_odometer']))\n\n    #odometer中剩余不能匹配yaer的NAN，使用均值进行填充\n    mean_avg_odometer = df1['avg_odometer'].mean()\n    df['odometer'] = df['odometer'].fillna(mean_avg_odometer)\n\n    odometer_data_new = df['odometer']\n    kmeans = KMeans(n_clusters=4)\n    kmeans.fit(odometer_data_new.values.reshape(-1, 1))\n    outliers = odometer_data_new[kmeans.labels_ == 1]\n    df = df[~df['odometer'].isin(outliers)]\n\n    #Use Kmeans to calculate outliers and delete them together with data with a price less than 1,000\n    #($100 is about ￥750. Even the price of a second-hand car is too low, so it is also regarded as an outlier and deleted)\n    price_data = df['price']\n    kmeans = KMeans(n_clusters=2)\n    kmeans.fit(price_data.values.reshape(-1, 1))\n    outliers = price_data[kmeans.labels_ == 1]\n\n    df = df[~((price_data.isin(outliers)) | (price_data < 100))]\n\n    #use linear regression to prediect values for null in year col.\n    features = [\"price\",'odometer']\n    df_complete = df.dropna(subset=[\"year\"] + features)\n    X_train = df_complete[features]\n    y_train = df_complete[\"year\"]\n    regression_model_year = LinearRegression()\n    regression_model_year.fit(X_train, y_train)\n    X_missing = df[df[\"year\"].isnull()][features]\n    if X_missing.shape[0] > 0:\n        predicted_year = regression_model_year.predict(X_missing)\n        df.loc[df[\"year\"].isnull(), \"year\"] = predicted_year.round().astype(int)\n   \n\n    #Fill null values using the modes of \"manufacturer\", \"cylinders\", 'fuel',\"title_status\", \"transmission\",\"drive\", \"type\", \"paint_color\", \"lat\", \"long\"\n    columns_to_fillna = ['manufacturer', 'cylinders', 'fuel', 'title_status', 'transmission',\n             'drive', 'type', 'paint_color', 'lat', 'long','posting_date']\n    modes = df[columns_to_fillna].mode().iloc[0]\n    df.loc[:, columns_to_fillna] = df.loc[:, columns_to_fillna].fillna(value=modes)\n\n    #for \"condition\" col: Fill NAN with randomly selected data from that column to manipulate the more possible results.\n    # Calculate the probability of each value (excluding NaN)\n    prob_values = df[df['condition'].notna()]['condition'].value_counts(normalize=True)\n    condition_options = df[\"condition\"].unique()\n    df.loc[df['condition'].isna(), 'condition'] = np.random.choice(prob_values.index, size=df['condition'].isna().sum(), p=prob_values.values)\n    df['condition'] = df['condition'].astype(str)\n\n\n    # Calculate the probability of each value (excluding 'other')\n    prob_values = df[df['cylinders'] != 'other']['cylinders'].value_counts(normalize=True)\n    # Assign 'other' values based on the probability\n    df.loc[df['cylinders'] == 'other', 'cylinders'] = np.random.choice(prob_values.index, size=df['cylinders'].eq('other').sum(), p=prob_values.values)\n    # Extract numeric values from the 'cylinder' column\n    df['cylinders'] = df['cylinders'].str.extract('(\\d+)').astype(int)\n\n\n    y = df['price']\n    x =df.drop(columns=['price'])\n\n\n    return x,y\n\ndef feature_engineering(df:pd.DataFrame)->pd.DataFrame:\n    \"\"\"\"\n    select useful feature\n    \"\"\"\n    #vehicle_age=posting_year - year, get the usage time of this vehicle\n    df[\"posting_date\"] = pd.to_datetime(df[\"posting_date\"], utc=True)\n    df[\"posting_year\"] = df[\"posting_date\"].dt.year\n    df[\"vehicle_age\"] = df[\"posting_year\"] - df[\"year\"]\n    \n    #Remove meaningless columns\n    #modify\n    df.drop(['id','url','region_url','model','title_status',\n         'image_url','VIN','size','county','description',\n         'lat','long','posting_date','posting_year'], \n        axis=1, inplace=True)\n    \n    return df\n\ndef encoding(df:pd.DataFrame,encoders=None)->pd.DataFrame:\n    cols = df.columns.values.tolist()\n    numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    categorical_cols = []\n    for col in cols:\n        if df[col].dtype in numerics: continue\n        categorical_cols.append(col)\n\n    if(encoders==None):\n        encoders=[]\n        for col in categorical_cols:\n            le = preprocessing.LabelEncoder()\n            le.fit(list(df[col].astype(str).values))\n            df[col] = le.transform(list(df[col].astype(str).values))\n            encoders.append(le)\n    else:\n        id=0\n        for col in categorical_cols:\n            le = encoders[id]\n            df[col] = le.transform(list(df[col].astype(str).values))\n            id=id+1\n\n    return df,encoders\n\n\n\ndef data_preprocessing(df:pd.DataFrame ,datasetType=\"train\", labelEncoders=None)->pd.DataFrame:\n    x , y =data_preparation(df)\n    x=feature_engineering(x)\n    encoders=labelEncoders\n    if(datasetType==\"train\"):\n        x,encoders=encoding(x,encoders=encoders)\n    if(datasetType==\"test\"):\n        if(encoders==None):\n            print(\"Please give a encoder\")\n            raise RuntimeError\n        x,_ = encoding(x,encoders)\n    return x,y,encoders","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:46:35.965602Z","iopub.execute_input":"2024-04-05T15:46:35.966442Z","iopub.status.idle":"2024-04-05T15:46:36.003951Z","shell.execute_reply.started":"2024-04-05T15:46:35.966404Z","shell.execute_reply":"2024-04-05T15:46:36.002782Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"df_train_x,df_train_y,encoders = data_preprocessing(df_train,datasetType=\"train\")\ndf_test_x, df_test_y,_ = data_preprocessing(df_test,datasetType=\"test\",labelEncoders=encoders)\nprint(df_train_x.isna().sum())\nprint(df['type'].unique())\nprint(df_train_x.shape, df_train_y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:46:44.082432Z","iopub.execute_input":"2024-04-05T15:46:44.083169Z","iopub.status.idle":"2024-04-05T15:46:46.366150Z","shell.execute_reply.started":"2024-04-05T15:46:44.083132Z","shell.execute_reply":"2024-04-05T15:46:46.364880Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_encode.py:224\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_encode.py:164\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_encode.py:164\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_encode.py:158\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n","\u001b[0;31mKeyError\u001b[0m: 'savannah / hinesville'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_train_x,df_train_y,encoders \u001b[38;5;241m=\u001b[39m data_preprocessing(df_train,datasetType\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m df_test_x, df_test_y,_ \u001b[38;5;241m=\u001b[39m \u001b[43mdata_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdatasetType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlabelEncoders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_train_x\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n","Cell \u001b[0;32mIn[68], line 140\u001b[0m, in \u001b[0;36mdata_preprocessing\u001b[0;34m(df, datasetType, labelEncoders)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease give a encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     x,_ \u001b[38;5;241m=\u001b[39m \u001b[43mencoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x,y,encoders\n","Cell \u001b[0;32mIn[68], line 123\u001b[0m, in \u001b[0;36mencoding\u001b[0;34m(df, encoders)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_cols:\n\u001b[1;32m    122\u001b[0m         le \u001b[38;5;241m=\u001b[39m encoders[\u001b[38;5;28mid\u001b[39m]\n\u001b[0;32m--> 123\u001b[0m         df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df,encoders\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:139\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_encode.py:226\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n","\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 'savannah / hinesville'"],"ename":"ValueError","evalue":"y contains previously unseen labels: 'savannah / hinesville'","output_type":"error"}]},{"cell_type":"code","source":"df_train_x[0:4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_x[0:4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Training and Evaluation**","metadata":{}},{"cell_type":"code","source":"# training\nnumJobs=3\ncv=5\nparamgrid={\n                \"objective\": [\"mae\"],\n                \"n_estimators\": [500,750],\n                \"num_leaves\": [256],\n                \"subsample\": [1],\n                \"colsample_bytree\":[0.6],\n                \"learning_rate\": [0.001],\n            }\n\nmodel = lgb.LGBMRegressor(n_estimators= 500,random_state=4487,objective='mae',learning_rate=0.05,importance_type= \"gain\",device=\"gpu\")\nmodelCV= GridSearchCV(model,param_grid=paramgrid,n_jobs=numJobs,verbose=True,cv=cv)\nmodelCV.fit(df_train_x, df_train_y)\n\n#predict\ny_pred = modelCV.predict(df_test_x)\n\n#evaluation\n\n# RMSE\nrmse = np.sqrt(mean_squared_error(df_test_y, y_pred))\nprint(f\"Root Mean Squared Error: {rmse}\")\n\n# MAE\nmae = mean_absolute_error(df_test_y, y_pred)\nprint(f\"Mean Absolute Error: {mae}\")\n\n# R2\nr2 = r2_score(df_test_y, y_pred)\nprint(f\"R-squared: {r2}\")\n\n#Residual Analysis: \nresiduals = df_test_y - y_pred\n\nplt.scatter(y_pred, residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessor = make_column_transformer(\n    (OneHotEncoder(), [\"manufacturer\",'condition', 'cylinders', 'fuel', 'transmission', 'drive', 'type', 'paint_color' ]),\n    (StandardScaler(), [\"year\",\"odometer\",\"vehicle_age\"]),\n    remainder=\"passthrough\"\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_search(_model,_preprocessor,_param_dist):\n    pipeline = make_pipeline(_preprocessor, _model)\n    random = RandomizedSearchCV(pipeline, param_distributions=_param_dist, n_iter=100, cv=5, n_jobs=16)\n    random.fit(df_train_x, df_train_y)\n    # Print the best hyperparameters and the corresponding score\n    print(\"Best parameters: {}\".format(random.best_params_))\n    print(\"Best cross-validation score: {:.2f}\".format(random.best_score_))\n    return random\n\n\ndef grid_search(_model,_preprocessor,_param_grid):\n    # Define the Gradient Boosting regression model\n    # Define the pipeline to preprocess the features and fit the model\n    pipeline = make_pipeline(_preprocessor, _model)\n    grid = GridSearchCV(pipeline, param_grid=_param_grid, cv=5, n_jobs=16)\n    grid.fit(df_train_x, df_train_y)\n    # Print the best hyperparameters and the corresponding score\n    print(\"Best parameters: {}\".format(grid.best_params_))\n    print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n    return grid\n    \ndef test_score(_model,_preprocessor):\n    best_pipeline = make_pipeline(_preprocessor, _model)\n    best_pipeline.fit(df_train_x, df_train_y)\n    df_test_pred = best_pipeline.predict(df_test_x)\n    rmse = np.sqrt(mean_squared_error(df_test_y, df_test_pred))\n    mae = mean_absolute_error(df_test_y, df_test_pred)\n    r2 = r2_score(df_test_y, df_test_pred)\n    print(\"R2 score: {}\".format(r2))\n    print(\"RMSE: {}\".format(rmse))\n    print(\"MAE: {}\".format(mae))\ndef test_score_param(_grid):\n    df_test_pred = _grid.best_estimator_.predict(df_test_x)\n    rmse = np.sqrt(mean_squared_error(df_test_y, df_test_pred))\n    mae = mean_absolute_error(df_test_y, df_test_pred)\n    r2 = r2_score(df_test_y, df_test_pred)\n    print(\"R2 score: {}\".format(r2))\n    print(\"RMSE: {}\".format(rmse))\n    print(\"MAE: {}\".format(mae))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_dist = {\n        \"gradientboostingregressor__n_estimators\": sp_randint(50, 200),\n        \"gradientboostingregressor__max_depth\": sp_randint(5, 10),\n        \"gradientboostingregressor__learning_rate\": uniform(0.0, 1.0),\n        \"gradientboostingregressor__subsample\": uniform(0.0, 1.0),\n        \"gradientboostingregressor__loss\": ['squared_error', 'quantile', 'huber', 'absolute_error'],\n        \"gradientboostingregressor__min_samples_split\": sp_randint(2, 10),\n        \"gradientboostingregressor__min_samples_leaf\": sp_randint(1, 10),\n        \"gradientboostingregressor__max_features\": ['sqrt', 'log2', None],\n        \"gradientboostingregressor__min_impurity_decrease\": uniform(0.0, 0.1)\n    }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\n        \"gradientboostingregressor__n_estimators\": [120,128,135,145],\n        \"gradientboostingregressor__max_depth\": [6, 7, 8],\n        \"gradientboostingregressor__learning_rate\": [0.55,0.6366443216222826,0.7,0.75],\n        \"gradientboostingregressor__subsample\": [0.3, 0.39654278232127016, 0.5],\n        \"gradientboostingregressor__loss\": ['huber'],\n        \"gradientboostingregressor__min_samples_split\": [1,2,3],\n        \"gradientboostingregressor__min_samples_leaf\": [3,4,5],\n        \"gradientboostingregressor__max_features\": ['sqrt', 'log2', None],\n        \"gradientboostingregressor__min_impurity_decrease\": [0.8,0.09041586944937485, 0.1]\n    }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = GradientBoostingRegressor(\n        n_estimators=100,\n        max_depth=7,\n        learning_rate=1,\n        subsample=0.8,\n        loss='huber',\n        random_state=42\n    )\nbest_model_2=GradientBoostingRegressor(\n        n_estimators=128,\n        max_depth=7,\n        learning_rate=0.6366443216222826,\n        subsample=0.39654278232127016,\n        loss='huber',\n        min_samples_leaf=4,\n        min_impurity_decrease=0.09041586944937485,\n        min_samples_split=2\n)\nparam_grid_G={}\ngrid_G=grid_search(GradientBoostingRegressor(),preprocessor,param_grid_G)\ntest_score_param(grid_G)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRegressor\nparam_grid_X={}\ngrid_X=grid_search(XGBRegressor(),preprocessor,param_grid_X)\ntest_score_param(grid_X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid_L={}\ngrid_L=grid_search(LinearRegression(),preprocessor,param_grid_L)\ntest_score_param(grid_L)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training\nnumJobs=3\ncv=5\nparamgrid={\n                \"objective\": [\"mae\"],\n                \"n_estimators\": [500,750],\n                \"num_leaves\": [256],\n                \"subsample\": [1],\n                \"colsample_bytree\":[0.6],\n                \"learning_rate\": [0.001],\n            }\n\nmodel = lgb.LGBMRegressor(n_estimators= 500,random_state=4487,objective='mae',learning_rate=0.05,importance_type= \"gain\",device=\"gpu\")\nmodelCV= GridSearchCV(model,param_grid=paramgrid,n_jobs=numJobs,verbose=True,cv=cv)\nmodelCV.fit(df_train_x, df_train_y)\n\n#predict\ny_pred = modelCV.predict(df_test_x)\n\n#evaluation\n\n# RMSE\nrmse = np.sqrt(mean_squared_error(df_test_y, y_pred))\nprint(f\"Root Mean Squared Error: {rmse}\")\n\n# MAE\nmae = mean_absolute_error(df_test_y, y_pred)\nprint(f\"Mean Absolute Error: {mae}\")\n\n# R2\nr2 = r2_score(df_test_y, y_pred)\nprint(f\"R-squared: {r2}\")\n\n#Residual Analysis: \nresiduals = df_test_y - y_pred\n\nplt.scatter(y_pred, residuals)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}